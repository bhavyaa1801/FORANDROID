{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d89916-4d9a-4798-94a1-db944c71a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "📂 Enter the case folder name:  trainning_case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using log file: /home/jovyan/my_android_logs/CASE_FILES_raw_logs/trainning_case/resolved_dns_log.csv\n",
      "⚠️ No 'timestamp' column found — skipping time-based features.\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       203\n",
      "           1       1.00      1.00      1.00        97\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       1.00      1.00      1.00       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n",
      "📁 Saved flagged logs → /home/jovyan/my_android_logs/CASE_FILES_raw_logs/trainning_case/ml_flagged_suspicious.csv\n",
      "📁 Model saved at → /home/jovyan/my_android_logs/CASE_FILES_raw_logs/trainning_case/suspicious_model.pkl\n",
      "📁 Scaler saved at → /home/jovyan/my_android_logs/CASE_FILES_raw_logs/trainning_case/scaler.pkl\n",
      "📁 Ranked suspicious IPs saved → /home/jovyan/my_android_logs/CASE_FILES_raw_logs/trainning_case/ranked_suspicious_ips.csv\n",
      "🔝 Top 10 Most Suspicious IPs:\n",
      "                ip  suspicion_probability risk_level\n",
      "0      1.27.90.142                    1.0       High\n",
      "1  100.138.184.222                    1.0       High\n",
      "2   100.80.181.243                    1.0       High\n",
      "3  101.230.115.192                    1.0       High\n",
      "4   102.220.170.14                    1.0       High\n",
      "5   103.102.156.81                    1.0       High\n",
      "6   103.123.150.31                    1.0       High\n",
      "7     104.36.40.42                    1.0       High\n",
      "8  106.230.134.225                    1.0       High\n",
      "9    106.54.174.53                    1.0       High\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os                                                                                       # TO CREATE A LIST OF SUSPICIOUS IPs USING AI MODEL \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# === User Input - Case Folder ===\n",
    "base_path = \"/home/jovyan/my_android_logs/CASE_FILES_raw_logs\"\n",
    "case_name = input(\"📂 Enter the case folder name: \")\n",
    "\n",
    "case_folder = os.path.join(base_path, case_name)\n",
    "log_file = os.path.join(case_folder, \"resolved_dns_log.csv\")\n",
    "\n",
    "if not os.path.exists(log_file):\n",
    "    raise FileNotFoundError(f\"❌ Log file not found at: {log_file}\")\n",
    "\n",
    "print(f\"✅ Using log file: {log_file}\")\n",
    "\n",
    "# === Load the Log Data ===\n",
    "df = pd.read_csv(log_file)\n",
    "\n",
    "# === Initialize Feature List ===\n",
    "features = [\n",
    "    'flag_uncommon_tld',\n",
    "    'domain_count',\n",
    "    'ip_count',\n",
    "    'flag_foreign_ip',\n",
    "    'abuse_score'\n",
    "]\n",
    "for col in features :\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "# ===Timestamp-Based Features ===\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['flag_odd_hour'] = df['hour'].apply(lambda h: h < 5 or h > 23 if pd.notnull(h) else 0)\n",
    "\n",
    "    features += ['hour', 'dayofweek', 'is_weekend', 'flag_odd_hour']\n",
    "    print(\"⏱️ Timestamp-based features enabled.\")\n",
    "else:\n",
    "    print(\"⚠️ No 'timestamp' column found — skipping time-based features.\")\n",
    "\n",
    "# === Select Features and Target ===\n",
    "X = df[features].fillna(0)\n",
    "\n",
    "# === Ensure 'is_suspicious' column exists ===\n",
    "if 'is_suspicious' not in df.columns:\n",
    "    if 'ip' in df.columns and df['ip'].notna().any():\n",
    "        # Load master list of known suspicious IPs \n",
    "        master_list_path = os.path.join(case_folder, \"master_list.csv\")\n",
    "        if os.path.exists(master_list_path):\n",
    "            master_df = pd.read_csv(master_list_path)\n",
    "            master_ips = set(master_df[\"ip\"].dropna())\n",
    "            print(f\"✅ Loaded master IP list with {len(master_ips)} entries.\")\n",
    "        else:\n",
    "            master_ips = set()\n",
    "            print(\"⚠️ No master list found — assuming empty list.\")\n",
    "\n",
    "        df[\"is_suspicious\"] = df[\"ip\"].apply(lambda ip: ip in master_ips if pd.notna(ip) else False)\n",
    "        print(\"✅ 'is_suspicious' column created based on master list.\")\n",
    "    else:\n",
    "        print(\"⚠️ No IPs found — creating dummy 'is_suspicious' column (all 0).\")\n",
    "        df[\"is_suspicious\"] = 0\n",
    "\n",
    "y = df['is_suspicious'].astype(int)\n",
    "\n",
    "# === Scale the Features ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# === Train the Random Forest Model ===\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluate the Model ===\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# === Predict on All Logs ===\n",
    "df['predicted_suspicious'] = model.predict(X_scaled)\n",
    "df['suspicion_probability'] = model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# === Export Flagged Entries to Case Folder ===\n",
    "flagged_output = os.path.join(case_folder, \"ml_flagged_suspicious.csv\")\n",
    "df[df['predicted_suspicious'] == 1].to_csv(flagged_output, index=False)\n",
    "print(f\"📁 Saved flagged logs → {flagged_output}\")\n",
    "\n",
    "# === Save Model and Scaler to Case Folder ===\n",
    "model_path = os.path.join(case_folder, \"suspicious_model.pkl\")                          #change model name\n",
    "scaler_path = os.path.join(case_folder, \"scaler.pkl\")\n",
    "joblib.dump(model, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"📁 Model saved at → {model_path}\")\n",
    "print(f\"📁 Scaler saved at → {scaler_path}\")\n",
    "\n",
    "# === Rank IPs by Risk (and Timestamp if available) ===\n",
    "suspicious_df = df[df['predicted_suspicious'] == 1]\n",
    "\n",
    "# Group IPs by suspicion probability and earliest timestamp \n",
    "agg_dict = {'suspicion_probability': 'max'}\n",
    "if 'timestamp' in df.columns:\n",
    "    agg_dict['timestamp'] = 'min'\n",
    "\n",
    "ip_risk_scores = suspicious_df.groupby('ip').agg(agg_dict).reset_index()\n",
    "\n",
    "# Risk Level Assignment\n",
    "def risk_level(score):\n",
    "    if score >= 0.9:\n",
    "        return \"High\"\n",
    "    elif score >= 0.7:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "ip_risk_scores[\"risk_level\"] = ip_risk_scores[\"suspicion_probability\"].apply(risk_level)\n",
    "\n",
    "# Save Ranked IPs\n",
    "ranked_ip_path = os.path.join(case_folder, \"ranked_suspicious_ips.csv\")\n",
    "ip_risk_scores.to_csv(ranked_ip_path, index=False)\n",
    "\n",
    "print(f\"📁 Ranked suspicious IPs saved → {ranked_ip_path}\")\n",
    "print(\"🔝 Top 10 Most Suspicious IPs:\")\n",
    "print(ip_risk_scores.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb49160-49fe-43b2-98a6-341c97d159ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipwhois\n",
      "  Downloading ipwhois-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting dnspython (from ipwhois)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.12/site-packages (from ipwhois) (0.7.1)\n",
      "Downloading ipwhois-1.3.0-py2.py3-none-any.whl (70 kB)\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: dnspython, ipwhois\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ipwhois]m1/2\u001b[0m [ipwhois]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dnspython-2.7.0 ipwhois-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ipwhois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf04359e-e754-4d1d-8a7c-689ad4f8af8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No new suspicious IPs found. Master list unchanged.\n"
     ]
    }
   ],
   "source": [
    "import os, json, requests\n",
    "import pandas as pd                                                                 # TO GET DATA ABOUT SUSPICIOUS IP\n",
    "from ipwhois import IPWhois\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#to open folder remove ir later\n",
    "# base_path = \"/home/jovyan/my_android_logs/CASE_FILES_raw_logs\"\n",
    "# case_name = input(\"📂 Enter the case folder name: \")\n",
    "# case_folder = os.path.join(base_path, case_name)\n",
    "\n",
    "# === Paths ===\n",
    "case_cache_geo_path = os.path.join(case_folder, \"geo_cache.json\")\n",
    "case_cache_whois_path = os.path.join(case_folder, \"whois_cache.json\")\n",
    "master_geo_path = \"/home/jovyan/my_android_logs/master_suspicious_geo_cache.json\"\n",
    "master_whois_path = \"/home/jovyan/my_android_logs/master_suspicious_whois_cache.json\"\n",
    "master_report_path = \"/home/jovyan/my_android_logs/master_suspicious_ip_report.csv\"\n",
    "\n",
    "# === Extract case name\n",
    "case_name = os.path.basename(case_folder)\n",
    "\n",
    "# === Load Cache Safely ===\n",
    "def safe_load_json(path):\n",
    "    if not os.path.exists(path): return {}\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data if isinstance(data, dict) else {}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "geo_map = safe_load_json(case_cache_geo_path)\n",
    "whois_map = safe_load_json(case_cache_whois_path)\n",
    "master_geo_map = safe_load_json(master_geo_path)\n",
    "master_whois_map = safe_load_json(master_whois_path)\n",
    "\n",
    "# === Check and reset corrupted entries\n",
    "def clean_cache(cache_name, cache_dict):\n",
    "    corrupted_keys = [k for k, v in cache_dict.items() if not isinstance(v, dict)]\n",
    "    if corrupted_keys:\n",
    "        print(f\"❌ Corrupt entries found in {cache_name}\")                         \n",
    "        for k in corrupted_keys:\n",
    "            del cache_dict[k]\n",
    "        print(f\"✅ Removed {len(corrupted_keys)} corrupt entries from {cache_name}\")\n",
    "    return cache_dict\n",
    "\n",
    "geo_map = clean_cache(\"geo_map\", geo_map)\n",
    "master_geo_map = clean_cache(\"master_geo_map\", master_geo_map)\n",
    "whois_map = clean_cache(\"whois_map\", whois_map)\n",
    "master_whois_map = clean_cache(\"master_whois_map\", master_whois_map)\n",
    "\n",
    "# === Expiry Logic\n",
    "def is_expired(ts_str, days=365):\n",
    "    try:\n",
    "        return datetime.now() - datetime.fromisoformat(ts_str) > timedelta(days=days)\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# === GeoIP Lookup\n",
    "def geoip_lookup(ip):\n",
    "    if (\n",
    "        ip in geo_map and\n",
    "        isinstance(geo_map[ip], dict) and\n",
    "        not is_expired(geo_map[ip].get(\"timestamp\", \"\"))\n",
    "    ):\n",
    "        return geo_map[ip]\n",
    "    try:\n",
    "        r = requests.get(f\"http://ip-api.com/json/{ip}\", timeout=5).json()\n",
    "        result = {\n",
    "            \"lat\": r.get(\"lat\"),\n",
    "            \"lon\": r.get(\"lon\"),\n",
    "            \"country\": r.get(\"countryCode\"),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    except:\n",
    "        result = {\"lat\": None, \"lon\": None, \"country\": \"Unknown\", \"timestamp\": datetime.now().isoformat()}\n",
    "    geo_map[ip] = result\n",
    "    master_geo_map[ip] = result\n",
    "    return result\n",
    "\n",
    "# === WHOIS Lookup\n",
    "def get_whois_info(ip):\n",
    "    if (\n",
    "        ip in whois_map and\n",
    "        isinstance(whois_map[ip], dict) and\n",
    "        not is_expired(whois_map[ip].get(\"timestamp\", \"\"))\n",
    "    ):\n",
    "        return whois_map[ip]\n",
    "    try:\n",
    "        obj = IPWhois(ip)\n",
    "        result = obj.lookup_rdap()\n",
    "        info = {\n",
    "            \"asn\": result.get(\"asn\"),\n",
    "            \"asn_description\": result.get(\"asn_description\"),\n",
    "            \"org_name\": result[\"network\"].get(\"name\"),\n",
    "            \"cidr\": result[\"network\"].get(\"cidr\"),\n",
    "            \"start_address\": result[\"network\"].get(\"start_address\"),\n",
    "            \"end_address\": result[\"network\"].get(\"end_address\"),\n",
    "            \"created\": str(result[\"network\"].get(\"created\")),\n",
    "            \"updated\": str(result[\"network\"].get(\"updated\")),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    except:\n",
    "        info = {k: None for k in [\n",
    "            \"asn\", \"asn_description\", \"org_name\", \"cidr\",\n",
    "            \"start_address\", \"end_address\", \"created\", \"updated\"\n",
    "        ]}\n",
    "        info[\"timestamp\"] = datetime.now().isoformat()\n",
    "    whois_map[ip] = info\n",
    "    master_whois_map[ip] = info\n",
    "    return info\n",
    "\n",
    "# === Process Only New Suspicious IPs\n",
    "suspicious_ips = df[df['predicted_suspicious'] == 1]['ip'].dropna().unique()\n",
    "existing_master_ips = set(master_geo_map.keys())\n",
    "\n",
    "new_ips = [ip for ip in suspicious_ips if ip not in existing_master_ips]\n",
    "\n",
    "if not new_ips:\n",
    "    print(\"✅ No new suspicious IPs found. Master list unchanged.\")\n",
    "else:\n",
    "    rows = []\n",
    "    for ip in tqdm(new_ips, desc=\"🔍 Enriching New Suspicious IPs\"):\n",
    "        geo = geoip_lookup(ip)\n",
    "        whois = get_whois_info(ip)\n",
    "        rows.append({\n",
    "            \"ip\": ip,\n",
    "            \"country\": geo.get(\"country\"),\n",
    "            \"latitude\": geo.get(\"lat\"),\n",
    "            \"longitude\": geo.get(\"lon\"),\n",
    "            \"case_name\": case_name,\n",
    "            **whois\n",
    "        })\n",
    "\n",
    "    # Save per-case report\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(case_folder, \"suspicious_ip_geo_whois.csv\"), index=False)\n",
    "\n",
    "    # Append to master report\n",
    "    if os.path.exists(master_report_path):\n",
    "        master_df = pd.read_csv(master_report_path)\n",
    "        updated_df = pd.concat([master_df, pd.DataFrame(rows)], ignore_index=True)\n",
    "    else:\n",
    "        updated_df = pd.DataFrame(rows)\n",
    "    \n",
    "    updated_df.drop_duplicates(subset='ip', keep='last', inplace=True)\n",
    "    updated_df.to_csv(master_report_path, index=False)\n",
    "\n",
    "    print(f\"✅ Master list updated with {len(new_ips)} new IP(s) → master_suspicious_ip_report.csv\")\n",
    "\n",
    "# === Save linkage log if timestamp+IP info is available\n",
    "link_cols = ['timestamp', 'app_name', 'domain', 'ip', 'suspicion_probability']\n",
    "available_cols = [col for col in link_cols if col in df.columns]\n",
    "if available_cols:\n",
    "    df[available_cols].dropna().to_csv(\n",
    "        os.path.join(case_folder, \"linked_logs.csv\"), index=False)\n",
    "\n",
    "# === Save all cache files\n",
    "with open(case_cache_geo_path, 'w') as f: json.dump(geo_map, f)\n",
    "with open(case_cache_whois_path, 'w') as f: json.dump(whois_map, f)\n",
    "with open(master_geo_path, 'w') as f: json.dump(master_geo_map, f)\n",
    "with open(master_whois_path, 'w') as f: json.dump(master_whois_map, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7bf8fa-5a17-45e3-875d-7964d5eab58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
